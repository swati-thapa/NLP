{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_2_210151488.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Name: Swati Thapa\n",
        "\n",
        "# Student ID: 210151488"
      ],
      "metadata": {
        "id": "oJt7D1HRMos2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Improve pre-processing (10 marks)\n",
        "Using the pre-processing techniques you have learned in the module, improve the `pre_process` function above, which currently just tokenizes text based on white space.\n",
        "\n",
        "When developing, use the 90% train and 10% validation data split from the training file, using the first 360 lines from the training split and first 40 lines from the validation split, as per above. To check the improvements by using the different techniques, use the `compute_IR_evaluation_scores` function as above. The **mean rank** is the main metric you need to focus on improving throughout this assignment, where the target/best possible performance is **1** (i.e. all test/validation data character documents are closest to their corresponding training data character documents) and the worst is **16**. Initially the code in this template achieves a mean rank of **5.12**  and accuracy of **0.3125** on the test set- you should be looking to improve those, particularly getting the mean rank as close to 1 as possible.\n"
      ],
      "metadata": {
        "id": "cUuro9F5DHi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!pip install num2words #Require to convert number to words"
      ],
      "metadata": {
        "id": "appb1dgfAwrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "Gn7V_mxcwn80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMABH1o29rzH"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "%matplotlib inline\n",
        "pd.options.display.max_colwidth=500\n",
        "import num2words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in training data and display in pandas dataframe\n",
        "train_path='training.csv'\n",
        "all_train_data = pd.read_csv(train_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "\n",
        "# Inspect\n",
        "all_train_data"
      ],
      "metadata": {
        "id": "fY-f_R9w91Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data.isnull().sum() #Observed 12 null values"
      ],
      "metadata": {
        "id": "jK9QjizPXyGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data.dropna(inplace=True) #Dropping rows conatining null"
      ],
      "metadata": {
        "id": "TWVY42kSX1kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test data for heldout validation with random samples of 9:1 train/heldout split\n",
        "from random import shuffle, seed\n",
        "\n",
        "seed(0) # set a seed for reproducibility so same split is used each time\n",
        "\n",
        "epsiode_scene_column = all_train_data.Episode.astype(str) + \"-\" + all_train_data.Scene.astype(str)\n",
        "all_train_data['episode_scene'] = epsiode_scene_column\n",
        "episode_scenes = sorted(list(set([x for x in epsiode_scene_column.values]))) # set function is random, need to sort!\n",
        "\n",
        "shuffle(episode_scenes)\n",
        "\n",
        "print(len(episode_scenes))\n",
        "episode_split = int(0.9*len(episode_scenes))\n",
        "training_ep_scenes = episode_scenes[:episode_split]\n",
        "test_ep_scenes = episode_scenes[episode_split:]\n",
        "print(len(training_ep_scenes), len(test_ep_scenes))\n",
        "\n",
        "def train_or_heldout_eps(val):\n",
        "    if val in training_ep_scenes:\n",
        "        return \"training\"\n",
        "    return \"heldout\"\n",
        "\n",
        "all_train_data['train_heldout'] = all_train_data['episode_scene'].apply(train_or_heldout_eps)"
      ],
      "metadata": {
        "id": "6L07P1Ib95QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Raw Data: ',np.shape(all_train_data))\n",
        "train_data = all_train_data[all_train_data['train_heldout']=='training']\n",
        "val_data = all_train_data[all_train_data['train_heldout']=='heldout']\n",
        "print('Train set: ',np.shape(train_data))\n",
        "print('Validation set: ',np.shape(val_data))"
      ],
      "metadata": {
        "id": "dalb-vTB98SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing used:\n",
        "a.\tLower casing all the words\n",
        "\n",
        "b.\tUsing different pattern to split properly with the help of regular expression:\n",
        "\n",
        "   •\tDidn’t use nltk default tokenization because it separate I’ll as I and ‘ll as two different words.\n",
        "\n",
        "   •\tSplitting use patter helps tokenizing properly word like I’ll and handle words like ..you.\n",
        "\n",
        "c.\tUsing stop words and punctuation to remove very common words and extra punctuations.\n",
        "\n",
        "d.\tConverting numeric to word eg: 25 as twenty five.\n",
        "\n",
        "e.\tConverting a word to its base form using lemmatization\n"
      ],
      "metadata": {
        "id": "thDemAHAm5L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "import re\n",
        "def pre_process(character_text):\n",
        "    \"\"\"Pre-process all the concatenated lines of a character, \n",
        "    using tokenization, spelling normalization and other techniques.\n",
        "    \n",
        "    Initially just a tokenization on white space. Improve this for Q1.\n",
        "    \n",
        "    ::character_text:: a string with all of one character's lines\n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer() #initializing lemmatizer\n",
        "    character_text=character_text.lower() #converting to all lower case\n",
        "    pattern = r\"\\s+|(?<=\\s)'|'(?=\\s)|'(?!.*\\.\\.) |(?<=\\w)([,..!/?])\" #different pattern to be used for splitting\n",
        "    result = [s for s in re.split(pattern, character_text) if s] \n",
        "    stop_words = stopwords.words('english') #usage of stopwords\n",
        "    punctuation=list(string.punctuation) #usage of punctuation to delete unwanted punctuation\n",
        "    punctuation.append(\"..\") #required punctuation has been appended\n",
        "    stop=stop_words+punctuation #now all words excepted stopwords and unwanted punctuation will be ignored\n",
        "    filtered_sentence = [w for w in result if not w in stop] \n",
        "    for i in filtered_sentence:\n",
        "        if i.isnumeric()==True:\n",
        "          indx=filtered_sentence.index(i)\n",
        "          k=num2words.num2words(i) #trying to convert numeric to word\n",
        "          filtered_sentence[indx]=k\n",
        "    lem_sent=[] \n",
        "    for i in filtered_sentence: #looping through each pre-processed word present in filtered_sentence\n",
        "      lem_sent.append(lemmatizer.lemmatize(i)) #Lemmitizing the word\n",
        "\n",
        "    return lem_sent"
      ],
      "metadata": {
        "id": "7EOtkNumRCVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example on how regex is working in our preprocess\n",
        "\n",
        "pattern = r\"\\s+|(?<=\\s)'|'(?=\\s)|'(?!.*\\.\\.) |(?<=\\w)([,..!/?])\"\n",
        "words = \"\"\"hello my name is 'joe..' what's your's\"\"\"\n",
        "result = [s for s in re.split(pattern, words) if s]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "0pFSx2ir4L05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### It can be observed words like \"i'm\" is restored. According to my observation I have observed if I use normal word.tokenize it split the word but even word like i'll becomes as i and 'll.\n",
        "For instance"
      ],
      "metadata": {
        "id": "6qV9E3avVidb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_data = \"I'll be at home. Where do you want to go? Oh no.....\"\n",
        "\n",
        "nltk_tokens = nltk.word_tokenize(word_data)\n",
        "print (nltk_tokens)"
      ],
      "metadata": {
        "id": "7ecwQl4eWP0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = pre_process(word_data) #demo of pre-process function\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "h0aPaxmaWbaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one document per character\n",
        "def create_character_document_from_dataframe(df, max_line_count):\n",
        "    \"\"\"Returns a dict with the name of the character as key,\n",
        "    their lines joined together as a single string, with end of line _EOL_\n",
        "    markers between them.\n",
        "    \n",
        "    ::max_line_count:: the maximum number of lines to be added per character\n",
        "    \"\"\"\n",
        "    character_docs = {}\n",
        "    character_line_count = {}\n",
        "    for line, name, gender in zip(df.Line, df.Character_name, df.Gender):\n",
        "        if not name in character_docs.keys():\n",
        "            character_docs[name] = \"\"\n",
        "            character_line_count[name] = 0\n",
        "        if character_line_count[name]==max_line_count:\n",
        "            continue\n",
        "        character_docs[name] += str(line)   + \" _EOL_ \"  # adding an end-of-line token\n",
        "        #character_docs[name] += str(line)   + \" \"  # adding an end-of-line token\n",
        "        character_line_count[name]+=1\n",
        "    print(\"lines per character\", character_line_count)\n",
        "    return character_docs"
      ],
      "metadata": {
        "id": "6UUSreBP9-7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the number of words each character has in the training set\n",
        "# only use the first 360 lines of each character\n",
        "train_character_docs = create_character_document_from_dataframe(train_data, max_line_count=360)\n",
        "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in train_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(train_character_docs[name].split()))\n",
        "    total_words += len(train_character_docs[name].split())\n",
        "print(\"total words\", total_words)"
      ],
      "metadata": {
        "id": "g4vvionu-F1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of pairs of (character name, pre-processed character) \n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n",
        "train_labels = [name for name, doc in training_corpus]"
      ],
      "metadata": {
        "id": "kKd9LsIjGwha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_character_docs = create_character_document_from_dataframe(val_data, max_line_count=40)\n",
        "print('Num. Characters: ',len(val_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in val_character_docs.keys():\n",
        "    print(name, 'Num of Words: ',len(val_character_docs[name].split()))\n",
        "    total_words += len(val_character_docs[name].split())\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "val_corpus = [(name, pre_process(doc)) for name, doc in sorted(val_character_docs.items())]\n",
        "val_labels = [name for name, doc in val_corpus]"
      ],
      "metadata": {
        "id": "HX5EVcSCKvNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_feature_vector_dictionary(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "    counts = Counter(character_doc)  # for now a simple count\n",
        "    counts = dict(counts)\n",
        "    # add the extra features, for now just adding one count for each extra feature\n",
        "    for feature in extra_features:\n",
        "        counts[feature] += 1\n",
        "    return counts  "
      ],
      "metadata": {
        "id": "3vzJBkKf-lhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "Ji8qfO9cC9Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(v1, v2):\n",
        "    \"\"\"Takes a pair of vectors v1 and v2 (1-d arrays e.g. [0, 0.5, 0.5])\n",
        "    returns the cosine similarity between the vectors\n",
        "    \"\"\"\n",
        "    \n",
        "    # compute cosine similarity manually\n",
        "    manual_cosine_similarity = np.dot(v1, v2)  /(norm(v1) * norm(v2))\n",
        "    \n",
        "    return manual_cosine_similarity"
      ],
      "metadata": {
        "id": "cdUYdERjF359"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_IR_evaluation_scores(train_feature_matrix, test_feature_matrix, train_labels, test_labels):\n",
        "    \"\"\"\n",
        "    Computes an information retrieval based on training data feature matrix and test data feature matrix\n",
        "    returns 4-tuple:\n",
        "    ::mean_rank:: mean of the ranking of the target document in terms of similarity to the query/test document\n",
        "    1 is the best possible score.\n",
        "    ::mean_cosine_similarity:: mean cosine similarity score for the target document vs. the test document of the same class\n",
        "    ::accuracy:: proportion of test documents correctly classified\n",
        "    ::df:: a data frame with all the similarity measures of the test documents vs. train documents\n",
        "    \n",
        "    params:\n",
        "    ::train_feature_matrix:: a numpy matrix N x M shape where N = number of characters M = number of features\n",
        "    ::test_feature_matrix::  a numpy matrix N x M shape where N = number of characters M = number of features\n",
        "    ::train_labels:: a list of character names for the training data in order consistent with train_feature_matrix\n",
        "    ::test_labels:: a list of character names for the test data in order consistent with test_feature_matrix\n",
        "    \"\"\"\n",
        "    rankings = []\n",
        "    all_cosine_similarities = []\n",
        "    pairwise_cosine_similarity = []\n",
        "    pairs = []\n",
        "    correct = 0\n",
        "    for i, target in enumerate(test_labels):\n",
        "        # compare the left out character against the mean\n",
        "        idx = i \n",
        "        fm_1 = test_feature_matrix.toarray()[idx]\n",
        "        all_sims = {}\n",
        "        print(\"target:\", target)\n",
        "        for j, other in enumerate(train_labels):\n",
        "            fm_2 = train_feature_matrix.toarray()[j]\n",
        "            manual_cosine_similarity = compute_cosine_similarity(fm_1, fm_2)\n",
        "            pairs.append((target, other))\n",
        "            pairwise_cosine_similarity.append(manual_cosine_similarity)\n",
        "            if other == target:\n",
        "                all_cosine_similarities.append(manual_cosine_similarity)\n",
        "            all_sims[other] = manual_cosine_similarity\n",
        "\n",
        "            # print(target, other, manual_cosine_similarity)\n",
        "        sorted_similarities = sorted(all_sims.items(),key=lambda x:x[1],reverse=True)\n",
        "        # print(sorted_similarities)\n",
        "        ranking = {key[0]: rank for rank, key in enumerate(sorted_similarities, 1)}\n",
        "        # print(\"Ranking for target\", ranking[target])\n",
        "        if ranking[target] == 1:\n",
        "            correct += 1\n",
        "        rankings.append(ranking[target])\n",
        "        # print(\"*****\")\n",
        "    mean_rank = np.mean(rankings)\n",
        "    mean_cosine_similarity = np.mean(all_cosine_similarities)\n",
        "    accuracy = correct/len(test_labels)\n",
        "    print(\"mean rank\", np.mean(rankings))\n",
        "    print(\"mean cosine similarity\", mean_cosine_similarity)\n",
        "    print(correct, \"correct out of\", len(test_labels), \"/ accuracy:\", accuracy )\n",
        "    \n",
        "    # get a dafaframe showing all the similarity scores of training vs test docs\n",
        "    df = pd.DataFrame({'doc1': [x[0] for x in pairs], 'doc2': [x[1] for x in pairs],\n",
        "                       'similarity': pairwise_cosine_similarity})\n",
        "\n",
        "    # display characters which are most similar and least similar\n",
        "    df.loc[[df.similarity.values.argmax(), df.similarity.values.argmin()]]\n",
        "    return (mean_rank, mean_cosine_similarity, accuracy, df)"
      ],
      "metadata": {
        "id": "i_bhEE3R-NP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heat_map_similarity(df):\n",
        "    \"\"\"Takes a dataframe with header 'doc1, doc2, similarity'\n",
        "    Plots a heatmap based on the similarity scores.\n",
        "    \"\"\"\n",
        "    test_labels =  sorted(list(set(df.sort_values(['doc1'])['doc1'])))\n",
        "    # add padding 1.0 values to either side\n",
        "    cm = [[1.0,] * (len(test_labels)+2)]\n",
        "    for target in test_labels:\n",
        "        new_row = [1.0]\n",
        "        for x in df.sort_values(['doc1', 'doc2'])[df['doc1']==target]['similarity']:\n",
        "            new_row.append(x)\n",
        "        new_row.append(1.0)\n",
        "        cm.append(new_row)\n",
        "    cm.append([1.0,] * (len(test_labels)+2))\n",
        "    #print(cm)\n",
        "    labels = [\"\"] + test_labels + [\"\"]\n",
        "    fig = plt.figure(figsize=(20,20))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Similarity matrix between documents as vectors')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks(np.arange(len(labels)))\n",
        "    ax.set_yticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels( labels, rotation=45)\n",
        "    ax.set_yticklabels( labels)\n",
        "\n",
        "    for i in range(len(cm)):\n",
        "        for j in range(len(cm)):\n",
        "\n",
        "            text = ax.text(j, i, round(cm[i][j],3),\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    plt.xlabel('Training Vector Doc')\n",
        "    plt.ylabel('Test Vector Doc')\n",
        "    #fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Kx-6t8PYOPlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "rrUAxvSsOS_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_heat_map_similarity(df)"
      ],
      "metadata": {
        "id": "iP6HHzZgXDgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result:\n",
        "Achieved 2.375 mean rank"
      ],
      "metadata": {
        "id": "CopxwkmnmE2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Improve linguistic feature extraction (15 marks)\n",
        "Use the feature extraction techniques you have learned to improve the `to_feature_vector_dictionary` function above. Examples of extra features could include extracting n-grams of different lengths and including POS-tags. You could also use sentiment analysis and gender classification (using the same data) as additional features.\n",
        "\n",
        "You could use some feature selection/reduction with techniques like minimum document frequency and/or feature selection like k-best selection using different criteria https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html. Again, develop on 90% training and 10% validation split and note the effect/improvement in mean rank with the techniques you use."
      ],
      "metadata": {
        "id": "kiZTR9-ccjF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Using count, one previous words and POS tag"
      ],
      "metadata": {
        "id": "POjOeoEW63eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features={} #Dictionary to store mean rank of each feature combination"
      ],
      "metadata": {
        "id": "JCBFDx8y-qJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    features_list=[] #Main list\n",
        "    features_dict={} #main feature dict\n",
        "    counts = Counter(character_doc)  #simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[] #storing PRE and POS\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #Checking first word condtion in a sentence \n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "            \n",
        "            \n",
        "\n",
        "       # For POS tag\n",
        "            pos_list=[] \n",
        "            pos_list.append(pos_tag([character_doc[i]])) \n",
        "            #pos_list format will be [[(\"hello\",\"NN\"),(\"go\",\"JJ\")]]\n",
        "            pos_dict={}\n",
        "            for m in pos_list:\n",
        "              pos_dict[m[0][0]]=m[0][1]\n",
        "            #pos_dict format will be {\"hello\": \"NN\", \"go\": \"JJ\"}\n",
        "            for key_1,value_1 in  pos_dict.items():\n",
        "              lis_values.append(\"POS_\" + str(value_1))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "\n",
        "    for i in list(features_dict.keys()): #Deleting _eol_ as it is redundant and have used just for indicating new line\n",
        "      if i=='_eol_':\n",
        "        del features_dict[i]\n",
        "\n",
        "    print(features_dict)\n",
        "    return (features_dict)"
      ],
      "metadata": {
        "id": "jrtnXdsY7M34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary(doc,[name]) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc,[name]) for name, doc in corpus])\n",
        "    \n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "print('\\n')\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "nHEJjjnb7RE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "bB1iCNlR7V3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[\"Count + ONE previous word+ POS tag\"]=mean_rank"
      ],
      "metadata": {
        "id": "wiBTf5Go_rul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Using count, one previous word, one next word and POS tag"
      ],
      "metadata": {
        "id": "Rw8c9wa9K4xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_2(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      \n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "            \n",
        "\n",
        "            if ((i>=len(character_doc)-1)|(character_doc[i]=='_eol_')): #for post word\n",
        "              next_word=\" \"\n",
        "            else:\n",
        "              next_word=character_doc[i+1]\n",
        "              if next_word=='_eol_':\n",
        "                next_word=\" \"\n",
        "            lis_values.append(\"POST_\" +str(next_word))\n",
        "            \n",
        "\n",
        "           # For POS tag\n",
        "            pos_list=[] \n",
        "            pos_list.append(pos_tag([character_doc[i]])) \n",
        "            #pos_list format will be [[(\"hello\",\"NN\"),(\"go\",\"JJ\")]]\n",
        "            pos_dict={}\n",
        "            for m in pos_list:\n",
        "              pos_dict[m[0][0]]=m[0][1]\n",
        "            #pos_dict format will be {\"hello\": \"NN\", \"go\": \"JJ\"}\n",
        "            for key_1,value_1 in  pos_dict.items():\n",
        "              lis_values.append(\"POS_\" + str(value_1))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "\n",
        "    for i in list(features_dict.keys()): #Deleting _eol_ as it is redundant and have used just for indicating new line\n",
        "      if i=='_eol_':\n",
        "        del features_dict[i]\n",
        "\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "b3ppWsiGHpQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "   \n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary_2(doc,[name]) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_2(doc,[name]) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "#print(training_corpus)\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "print('\\n')\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "RJe_vrmA7ua5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "cCwbq9oz70ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[\"Count + ONE previous word+ ONE next word+ POS tag\"]=mean_rank"
      ],
      "metadata": {
        "id": "tD4rCzG8_3Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 One previous and one next word"
      ],
      "metadata": {
        "id": "zp3GA9B86I5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_3(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for now a simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      \n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "            \n",
        "\n",
        "            if ((i>=len(character_doc)-1)|(character_doc[i]=='_eol_')): #for post word\n",
        "              next_word=\" \"\n",
        "            else:\n",
        "              next_word=character_doc[i+1]\n",
        "              if next_word=='_eol_':\n",
        "                next_word=\" \"\n",
        "            lis_values.append(\"POST_\" +str(next_word))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "      for i in list(features_dict.keys()): #Deleting _eol_ as it is redundant and have used just for indicating new line\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "bHNrHgSM6Ilv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "   \n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary_3(doc,[name]) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_3(doc,[name]) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "#print(training_corpus)\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "print('\\n')\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "umavZuid6SxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "6q-nAxUq6WcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[\"Count +ONE previous word + ONE next word\"]=mean_rank"
      ],
      "metadata": {
        "id": "oQQD55id__3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Bigrams:- Two previous words and POS tag"
      ],
      "metadata": {
        "id": "YKjqQGMdtLnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_4(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "  \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for simple count\n",
        "    counts=dict(counts)\n",
        "\n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word_1=\" \"\n",
        "              prev_word_2=\" \"\n",
        "            elif (character_doc[i-2]==character_doc[-1])|(character_doc[i-2]=='_eol_'):\n",
        "              prev_word_1=character_doc[i-1]\n",
        "              prev_word_2=\" \"\n",
        "            else:\n",
        "              prev_word_1=character_doc[i-1]\n",
        "              prev_word_2=character_doc[i-2]\n",
        "            lis_values.append(\"PRE1_\"+str(prev_word_1))\n",
        "            lis_values.append(\"PRE2_\"+str(prev_word_2))\n",
        "\n",
        "            pos_list=[]\n",
        "            pos_list.append(pos_tag([character_doc[i]]))\n",
        "\n",
        "            pos_dict={} #For POS tag\n",
        "            for m in pos_list:\n",
        "              pos_dict[m[0][0]]=m[0][1]\n",
        "            for key_1,value_1 in  pos_dict.items():\n",
        "              lis_values.append(\"POS_\" + str(value_1))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "      for i in list(features_dict.keys()): #Deleting _eol_ as it is redundant and have used just for indicating new line\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "LzYguMXttKnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    #print(len(corpus))\n",
        "    # for i in range(0,len(corpus)):\n",
        "    #   print(corpus[i][0])\n",
        "    #print(corpus[2][0])\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary_4(doc,[name]) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_4(doc,[name]) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "#print(training_corpus)\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "print('\\n')\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "u1ozqJQnHpLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "9yD9MD_zHpGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[\"Count + Two previous word(Bi-grams)\"]=mean_rank"
      ],
      "metadata": {
        "id": "2q2jxTACAQ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Testing on best feature combination:\n",
        "i.e \"Count + ONE previous word+ ONE next word+ POS tag\""
      ],
      "metadata": {
        "id": "8TZ9TkZO4F1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_5(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    #print(character_doc)\n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "    pos=dict(pos_tag(character_doc))\n",
        "    features_dict={}\n",
        "\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      if key in pos: #Adding just one POS tag\n",
        "        lis_values.append(\"POS_\"+str(pos[key]))\n",
        "      \n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "\n",
        "            if ((i>=len(character_doc)-1)|(character_doc[i]=='_eol_')): #for post word\n",
        "              next_word=\" \"\n",
        "            else:\n",
        "              next_word=character_doc[i+1]\n",
        "              if next_word=='_eol_':\n",
        "                next_word=\" \"\n",
        "            lis_values.append(\"POST_\" +str(next_word))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "      for i in list(features_dict.keys()):\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "xVdG0LigX8RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    #print(len(corpus))\n",
        "    # for i in range(0,len(corpus)):\n",
        "    #   print(corpus[i][0])\n",
        "    #print(corpus[2][0])\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary_5(doc,[name]) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_5(doc,[name]) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "#print(training_corpus)\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "print('\\n')\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "VZARmM2f8rtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "aAs2gkTX8y92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features[\"Count + ONE previous word+ ONE next word+ POS tag (only one)\"]=mean_rank"
      ],
      "metadata": {
        "id": "TxsZrGGSAcKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "h_sRSTiiSy7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "u1L5efdZAt-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pylab import *\n",
        "fig = figure()\n",
        "names = list(features.keys())\n",
        "values = list(features.values())\n",
        "plt.bar(0,values[0],tick_label=names[0])\n",
        "plt.bar(1,values[1],tick_label=names[1])\n",
        "plt.bar(2,values[2],tick_label=names[2])\n",
        "plt.bar(3,values[3],tick_label=names[3])\n",
        "plt.bar(4,values[4],tick_label=names[4])\n",
        "plt.xticks(range(0,5),names)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xticklabels(features.keys(),rotation=90) ;\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lKbJPd1XBJU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result: \n",
        "\n",
        "From above features “Count + ONE previous word+ ONE next word+ POS tag (only one)” gave the best mean rank of 1.812"
      ],
      "metadata": {
        "id": "52iNvi4pDHV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. Add dialogue context data and features (15 marks)\n",
        "Adjust `create_character_document_from_dataframe` and the other functions appropriately so the data incorporates the context of the line spoken by the characters in terms of the lines spoken by other characters in the same scene (immediately before and after). You can also use **scene information** from the other columns **(but NOT the gender and character names directly)**."
      ],
      "metadata": {
        "id": "72XLv7jwxJPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in training data and display in pandas dataframe\n",
        "train_path='training.csv'\n",
        "all_train_data = pd.read_csv(train_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "\n",
        "# Inspect\n",
        "all_train_data"
      ],
      "metadata": {
        "id": "9YJFI2mDxDpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test data for heldout validation with random samples of 9:1 train/heldout split\n",
        "from random import shuffle, seed\n",
        "\n",
        "seed(0) # set a seed for reproducibility so same split is used each time\n",
        "\n",
        "epsiode_scene_column = all_train_data.Episode.astype(str) + \"-\" + all_train_data.Scene.astype(str)\n",
        "all_train_data['episode_scene'] = epsiode_scene_column\n",
        "episode_scenes = sorted(list(set([x for x in epsiode_scene_column.values]))) # set function is random, need to sort!\n",
        "\n",
        "shuffle(episode_scenes)\n",
        "\n",
        "print(len(episode_scenes))\n",
        "episode_split = int(0.9*len(episode_scenes))\n",
        "training_ep_scenes = episode_scenes[:episode_split]\n",
        "test_ep_scenes = episode_scenes[episode_split:]\n",
        "print(len(training_ep_scenes), len(test_ep_scenes))\n",
        "\n",
        "def train_or_heldout_eps(val):\n",
        "    if val in training_ep_scenes:\n",
        "        return \"training\"\n",
        "    return \"heldout\"\n",
        "\n",
        "all_train_data['train_heldout'] = all_train_data['episode_scene'].apply(train_or_heldout_eps)"
      ],
      "metadata": {
        "id": "w4EorqJoxDnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Raw Data: ',np.shape(all_train_data))\n",
        "train_data = all_train_data[all_train_data['train_heldout']=='training']\n",
        "val_data = all_train_data[all_train_data['train_heldout']=='heldout']\n",
        "print('Train set: ',np.shape(train_data))\n",
        "print('Validation set: ',np.shape(val_data))"
      ],
      "metadata": {
        "id": "0UvcAe5IxDkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "import re\n",
        "def pre_process(character_text):\n",
        "    \"\"\"Pre-process all the concatenated lines of a character, \n",
        "    using tokenization, spelling normalization and other techniques.\n",
        "    \n",
        "    Initially just a tokenization on white space. Improve this for Q1.\n",
        "    \n",
        "    ::character_text:: a string with all of one character's lines\n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer() #initializing lemmatizer\n",
        "    character_text=character_text.lower() #converting to all lower case\n",
        "    pattern = r\"\\s+|(?<=\\s)'|'(?=\\s)|'(?!.*\\.\\.) |(?<=\\w)([,..!/?])\" #different pattern to be used for splitting\n",
        "    result = [s for s in re.split(pattern, character_text) if s] \n",
        "    stop_words = stopwords.words('english') #usage of stopwords\n",
        "    punctuation=list(string.punctuation) #usage of punctuation to delete unwanted punctuation\n",
        "    punctuation.append(\"..\") #required punctuation has been appended\n",
        "    stop=stop_words+punctuation #now all words excepted stopwords and unwanted punctuation will be ignored\n",
        "    filtered_sentence = [w for w in result if not w in stop] \n",
        "    for i in filtered_sentence:\n",
        "        if i.isnumeric()==True:\n",
        "          indx=filtered_sentence.index(i)\n",
        "          k=num2words.num2words(i) #trying to convert numeric to word\n",
        "          filtered_sentence[indx]=k\n",
        "    lem_sent=[] \n",
        "    for i in filtered_sentence: #looping through each pre-processed word present in filtered_sentence\n",
        "      lem_sent.append(lemmatizer.lemmatize(i)) #Lemmitizing the word\n",
        "\n",
        "    return lem_sent"
      ],
      "metadata": {
        "id": "Rk1qTdk2xDhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "1itnEJnkcnAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data.isnull().sum()"
      ],
      "metadata": {
        "id": "5Fp_NQf-ZNuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.dropna(inplace=True) #Observed null value in train data therefore dropping row which has null"
      ],
      "metadata": {
        "id": "94ByNZSLdAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "8nuA95BhdDlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "2pgimuu7iiME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some pre- processing such as stop word and punctuation removal have been done in “Line” column as we are focusing on the number of counts as feature in this question.\n",
        "Reason to take number of counts:\n",
        "\n",
        "a.\tSince this is drama one character calling other character name in the line is common. It becomes easier to know at least the current character won’t be repeating his/her name again and again in whole drama.\n",
        "\n",
        "b.\tEasy to count scene_info and find a pattern which character has more chance of playing the scene\n"
      ],
      "metadata": {
        "id": "v32Q3J6WRriU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "punctuation=list(string.punctuation)\n",
        "punctuation.append(\"..\")\n",
        "stop=stop_words+punctuation"
      ],
      "metadata": {
        "id": "yGgDoLqOjojZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "1c5x65C_5aHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['Line'] = train_data['Line'].apply(word_tokenize)\n",
        "train_data['Line'] = train_data['Line'].apply(lambda words: [word for word in words if word not in stop])"
      ],
      "metadata": {
        "id": "I79Fz4ntomFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data['Line'] = val_data['Line'].apply(word_tokenize) #preprocessing validation set too\n",
        "val_data['Line'] = val_data['Line'].apply(lambda words: [word for word in words if word not in stop])"
      ],
      "metadata": {
        "id": "QAIR1CTZZTsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After preprocessing \"Line\" column joining the word as senetence again in train data\n",
        "no_stop=[]\n",
        "for i in train_data['Line']:\n",
        "  k=' '.join(i)\n",
        "  no_stop.append(k)\n",
        "train_data['Line']=no_stop"
      ],
      "metadata": {
        "id": "MS3uECE3joLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After preprocessing \"Line\" column joining the word as senetence again in val data\n",
        "no_stop=[]\n",
        "for i in val_data['Line']:\n",
        "  k=' '.join(i)\n",
        "  no_stop.append(k)\n",
        "val_data['Line']=no_stop"
      ],
      "metadata": {
        "id": "DW59hot-ZcAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "58LxZS_woeKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "id": "eauTs3cKoU09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.reset_index() #resetting the index as some part from all_train_data were taken as validation set\n",
        "train_data.drop(['index'],axis=1,inplace=True)\n",
        "train_data"
      ],
      "metadata": {
        "id": "qasZ95vCNo2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = val_data.reset_index() #resetting the index as some part from all_train_data were taken as train set\n",
        "val_data.drop(['index'],axis=1,inplace=True)\n",
        "val_data"
      ],
      "metadata": {
        "id": "V1KRNqZCZ5ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this problem as mentioned in question I have made a column “pre_current_post” in data frame which includes scene info, previous line and post line for a current character line provided they are in same scene and are different character. "
      ],
      "metadata": {
        "id": "Z4quxtvoRWiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditions covered:\n",
        "\n",
        "a.\tIf current line is the first line of a scene or from same character in the scene then it returns “NONE” or each word is concatenated with PRE_ after tokenizing\n",
        "\n",
        "b.\tIf current line is the last line of a scene or from same character in the scene then it returns “NONE” or each word is concatenated with POST_ after tokenizing \n",
        "\n",
        "c.\tCurrent line is tokenized and don’t have any prefix.\n",
        "\n",
        "d.\tScene info have been concatenated with “_” to make it look like one single word.\n",
        "\n",
        "e.\t_EOL_ is added at the end to know current line pre-processing is done and new line start after that\n"
      ],
      "metadata": {
        "id": "h5tZ0wuFRfIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one document per character\n",
        "def create_character_document_from_dataframe(df, max_line_count):\n",
        "    \"\"\"Returns a dict with the name of the character as key,\n",
        "    their lines joined together as a single string, with end of line EOL\n",
        "    markers between them.\n",
        "    \n",
        "    ::max_line_count:: the maximum number of lines to be added per character\n",
        "    \"\"\"\n",
        "    #incorporates the context of the line spoken by the characters in terms of the lines spoken by other characters in the same scene (immediately before and after)\n",
        "    result = []\n",
        "    \n",
        "    Scene_info_list=[]\n",
        "    for i in df['Scene_info']: # few Scene_info \"SLATERS', KITCHEN/FRONT ROOM INT DAY LIGHT\" are in this format\n",
        "      k=i.replace(\",\",\"\") \n",
        "      k=k.replace(' ','_')# therefore making it into \"SLATERS'_KITCHEN/FRONT_ROOM_INT_DAY_LIGHT\" format\n",
        "      Scene_info_list.append(k)\n",
        "    df['Scene_info']=Scene_info_list\n",
        "    episodeCol = df.Episode\n",
        "    sceneCol = df.Scene\n",
        "    lineCol = df.Line\n",
        "    charname=df.Character_name\n",
        "    episcene= df.episode_scene\n",
        "    scene_info=df.Scene_info\n",
        "\n",
        "\n",
        "    currEpisode = episodeCol[0]\n",
        "    currScene = sceneCol[0]\n",
        "      \n",
        "    for i in range(0, len(episodeCol)): \n",
        "        temp = []\n",
        "\n",
        "        ep = episodeCol[i]\n",
        "        scene = sceneCol[i]\n",
        "        line = lineCol[i]\n",
        "        name=charname[i]\n",
        "        episcene_con=episcene[i]\n",
        "        scene_info_con=scene_info[i]\n",
        "\n",
        "        lineList = line.split()\n",
        "\n",
        "        if(i != 0): #For previous character line\n",
        "          prevEp = episodeCol[i-1]\n",
        "          prevScene = sceneCol[i-1]\n",
        "          prevLine = lineCol[i-1]\n",
        "          prevChar = charname[i-1]\n",
        "        else:\n",
        "          prevEp = -1\n",
        "          prevScene = -1\n",
        "          prevLine = 'NONE'  \n",
        "          prevChar = -1      \n",
        "\n",
        "\n",
        "        prevLineList = prevLine.split()\n",
        "\n",
        "        if(i != len(episodeCol)-1): #for post character line\n",
        "          nextEp = episodeCol[i+1]\n",
        "          nextScene = sceneCol[i+1]\n",
        "          nextLine = lineCol[i+1]  \n",
        "          nextChar = charname[i+1]\n",
        "        else:\n",
        "          nextEp = -1\n",
        "          nextScene = -1\n",
        "          nextLine = 'NONE'  \n",
        "          nextChar = -1\n",
        "\n",
        "\n",
        "        nextLineList = nextLine.split()\n",
        "\n",
        "\n",
        "        if(ep == prevEp and scene == prevScene and name!=prevChar): \n",
        "            append_str = 'PRE_'\n",
        "            pre_res = [append_str + sub for sub in prevLineList]\n",
        "            temp.append(scene_info_con)\n",
        "            temp.extend(pre_res)\n",
        "        else:\n",
        "            temp.append(scene_info_con)\n",
        "            temp.append('NONE')\n",
        "        \n",
        "\n",
        "        temp.extend(lineList)\n",
        "\n",
        "        if(ep == nextEp and scene == nextScene and name!=nextChar):\n",
        "            append_str = 'POST_'\n",
        "            pre_res = [append_str + sub for sub in nextLineList]\n",
        "            temp.extend(pre_res)\n",
        "\n",
        "        else:\n",
        "            temp.append('NONE')\n",
        "        \n",
        "        result.append(temp)\n",
        "        \n",
        "\n",
        "    df['pre_current_post']=result #make new column in datframe with pre line current line and post line\n",
        "    uni_dict={} #initialise unique dict\n",
        "    line_list=[] #initialise list\n",
        "    char_name=df['Character_name'].unique() #getting unique char name\n",
        "    for i in char_name: #for each char unique dataframe\n",
        "      char_name_dataframe=df.loc[df['Character_name']==i]\n",
        "      for pre_post in char_name_dataframe.pre_current_post:\n",
        "        pre_post.append('_EOL_') #EOL to know current line ending\n",
        "        line_list=line_list + pre_post\n",
        "        uni_dict[i]=line_list\n",
        "      line_list=[]\n",
        "\n",
        "    print(uni_dict)\n",
        "    return uni_dict"
      ],
      "metadata": {
        "id": "DXxVh52DMCP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the number of words each character has in the training set\n",
        "# only use the first 360 lines of each character\n",
        "train_character_docs = create_character_document_from_dataframe(train_data, max_line_count=360)\n",
        "for k,v in train_character_docs.items():\n",
        "  train_character_docs[k]=\",\".join(v)\n",
        "#train_character_docs\n",
        "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in train_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(train_character_docs[name].split(',')))\n",
        "    total_words += len(train_character_docs[name].split())\n",
        "print(\"total words\", total_words)"
      ],
      "metadata": {
        "id": "dmjRrx_CxDcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format example: \n",
        "\n",
        "a.\tDESERTED_CAR_PARK_EXT_NIGHT, NONE, Look, ya, mark, ya, And, think, 're, unlucky, man, POST_Shirl, POST_..., _EOL_         When current line is first line in scene\n",
        "\n",
        "b.\tR&R_INT_NIGHT,PRE_Okay,Are,alright,You,'ve,bit,since,got,POST_Are,POST_alright,_EOL_   General format when it satisfies our condition\n"
      ],
      "metadata": {
        "id": "ZtzSeSGVR6uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "id": "2e9kmzaSpqvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_character_docs"
      ],
      "metadata": {
        "id": "kSx6WgQtfWkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of pairs of (character name, pre-processed character) \n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n",
        "train_labels = [name for name, doc in training_corpus]"
      ],
      "metadata": {
        "id": "YcZiz3tsxDOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "id": "-IiLe4laUvg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_character_docs = create_character_document_from_dataframe(val_data, max_line_count=40)\n",
        "for k,v in val_character_docs.items():\n",
        "  val_character_docs[k]=\",\".join(v)\n",
        "val_character_docs\n",
        "print('Num. Characters: ',len(val_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in val_character_docs.keys():\n",
        "    print(name, 'Num of Words: ',len(val_character_docs[name].split(',')))\n",
        "    total_words += len(val_character_docs[name].split(','))\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "val_corpus = [(name, pre_process(doc)) for name, doc in sorted(val_character_docs.items())]\n",
        "val_labels = [name for name, doc in val_corpus]"
      ],
      "metadata": {
        "id": "UKXflTmuxDLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_feature_vector_dictionary(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "    counts = Counter(character_doc)  # for now a simple count\n",
        "    counts = dict(counts)\n",
        "    # add the extra features, for now just adding one count for each extra feature\n",
        "    for feature in extra_features:\n",
        "        counts[feature] += 1\n",
        "    \n",
        "    for i in list(counts.keys()): #deleting \"none\" and \"_eol_\" key as these don't have much meaning in the dictionary \n",
        "      if i=='none' or i=='_eol_':\n",
        "        del counts[i]\n",
        "    #print(counts)\n",
        "    return counts  "
      ],
      "metadata": {
        "id": "NACEbQSrafdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "01skp2BjxDGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "cMbzZoLeM2Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result:\n",
        "Achieved mean rank of 1.5"
      ],
      "metadata": {
        "id": "pzT99d2lSDfu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Improve the vectorization method (10 marks)\n",
        "Use a matrix transformation technique like TF-IDF (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to improve the `create_document_matrix_from_corpus` function, which currently only uses a dictionary vectorizor (`DictVectorizer`) which straight-forwardly maps from the feature dictionaries produced for each character document to a sparse matrix.\n",
        "\n",
        "As the `create_document_matrix_from_corpus` is designed to be used both in training/fitting (with `fitting` set to `True`) and in transformation alone on test/validation data (with `fitting` set to `False`), make sure you initialize any transformers you want to try in the same place as `corpusVectorizer = DictVectorizer()` before you call \n",
        "`create_document_matrix_from_corpus`. Again, develop on 90% training 10% validation split and note the effect/improvement in mean rank with each technique you try."
      ],
      "metadata": {
        "id": "Xwt3Ei1RMBAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Applying TF-IDF on q3 feature. i.e scene_info + previous line + current line + post line"
      ],
      "metadata": {
        "id": "JDeJ3VbpXc5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "iU182GzTWLja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "GP0O0mTvWOsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf={}\n",
        "features_tfidf[\"scene_info + previous line + current line + post line\"]=mean_rank"
      ],
      "metadata": {
        "id": "vtUno9rddFqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note:\n",
        "I'm running same steps as I have run in q1 and q2 because in q3 I had added new column and features were different too"
      ],
      "metadata": {
        "id": "lwi8zv1yXuQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in training data and display in pandas dataframe\n",
        "train_path='training.csv'\n",
        "all_train_data = pd.read_csv(train_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "\n",
        "# Inspect\n",
        "all_train_data"
      ],
      "metadata": {
        "id": "981q9S1OTYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data.dropna(inplace=True) #dropping rows which has null values"
      ],
      "metadata": {
        "id": "WNl3WHEmTYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test data for heldout validation with random samples of 9:1 train/heldout split\n",
        "from random import shuffle, seed\n",
        "\n",
        "seed(0) # set a seed for reproducibility so same split is used each time\n",
        "\n",
        "epsiode_scene_column = all_train_data.Episode.astype(str) + \"-\" + all_train_data.Scene.astype(str)\n",
        "all_train_data['episode_scene'] = epsiode_scene_column\n",
        "episode_scenes = sorted(list(set([x for x in epsiode_scene_column.values]))) # set function is random, need to sort!\n",
        "\n",
        "shuffle(episode_scenes)\n",
        "\n",
        "print(len(episode_scenes))\n",
        "episode_split = int(0.9*len(episode_scenes))\n",
        "training_ep_scenes = episode_scenes[:episode_split]\n",
        "test_ep_scenes = episode_scenes[episode_split:]\n",
        "print(len(training_ep_scenes), len(test_ep_scenes))\n",
        "\n",
        "def train_or_heldout_eps(val):\n",
        "    if val in training_ep_scenes:\n",
        "        return \"training\"\n",
        "    return \"heldout\"\n",
        "\n",
        "all_train_data['train_heldout'] = all_train_data['episode_scene'].apply(train_or_heldout_eps)"
      ],
      "metadata": {
        "id": "pFTHbRQUTYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Raw Data: ',np.shape(all_train_data))\n",
        "train_data = all_train_data[all_train_data['train_heldout']=='training']\n",
        "val_data = all_train_data[all_train_data['train_heldout']=='heldout']\n",
        "print('Train set: ',np.shape(train_data))\n",
        "print('Validation set: ',np.shape(val_data))"
      ],
      "metadata": {
        "id": "z__YQmuJTYXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "import re\n",
        "def pre_process(character_text):\n",
        "    \"\"\"Pre-process all the concatenated lines of a character, \n",
        "    using tokenization, spelling normalization and other techniques.\n",
        "    \n",
        "    Initially just a tokenization on white space. Improve this for Q1.\n",
        "    \n",
        "    ::character_text:: a string with all of one character's lines\n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer() #initializing lemmatizer\n",
        "    character_text=character_text.lower() #converting to all lower case\n",
        "    pattern = r\"\\s+|(?<=\\s)'|'(?=\\s)|'(?!.*\\.\\.) |(?<=\\w)([,..!/?])\" #different pattern to be used for splitting\n",
        "    result = [s for s in re.split(pattern, character_text) if s] \n",
        "    stop_words = stopwords.words('english') #usage of stopwords\n",
        "    punctuation=list(string.punctuation) #usage of punctuation to delete unwanted punctuation\n",
        "    punctuation.append(\"..\") #required punctuation has been appended\n",
        "    stop=stop_words+punctuation #now all words excepted stopwords and unwanted punctuation will be ignored\n",
        "    filtered_sentence = [w for w in result if not w in stop] \n",
        "    for i in filtered_sentence:\n",
        "        if i.isnumeric()==True:\n",
        "          indx=filtered_sentence.index(i)\n",
        "          k=num2words.num2words(i) #trying to convert numeric to word\n",
        "          filtered_sentence[indx]=k\n",
        "    lem_sent=[] \n",
        "    for i in filtered_sentence: #looping through each pre-processed word present in filtered_sentence\n",
        "      lem_sent.append(lemmatizer.lemmatize(i)) #Lemmitizing the word\n",
        "\n",
        "    return lem_sent"
      ],
      "metadata": {
        "id": "jH7_IFhkTYXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one document per character\n",
        "def create_character_document_from_dataframe(df, max_line_count):\n",
        "    \"\"\"Returns a dict with the name of the character as key,\n",
        "    their lines joined together as a single string, with end of line _EOL_\n",
        "    markers between them.\n",
        "    \n",
        "    ::max_line_count:: the maximum number of lines to be added per character\n",
        "    \"\"\"\n",
        "    character_docs = {}\n",
        "    character_line_count = {}\n",
        "    for line, name, gender in zip(df.Line, df.Character_name, df.Gender):\n",
        "        if not name in character_docs.keys():\n",
        "            character_docs[name] = \"\"\n",
        "            character_line_count[name] = 0\n",
        "        if character_line_count[name]==max_line_count:\n",
        "            continue\n",
        "        character_docs[name] += str(line)   + \" _EOL_ \"  # adding an end-of-line token\n",
        "        #character_docs[name] += str(line)   + \" \"  # adding an end-of-line token\n",
        "        character_line_count[name]+=1\n",
        "    print(\"lines per character\", character_line_count)\n",
        "    return character_docs"
      ],
      "metadata": {
        "id": "gdywKxmmTYXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the number of words each character has in the training set\n",
        "# only use the first 360 lines of each character\n",
        "train_character_docs = create_character_document_from_dataframe(train_data, max_line_count=360)\n",
        "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in train_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(train_character_docs[name].split()))\n",
        "    total_words += len(train_character_docs[name].split())\n",
        "print(\"total words\", total_words)"
      ],
      "metadata": {
        "id": "nM1DkVzxTYXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_character_docs"
      ],
      "metadata": {
        "id": "uLU_VsK0TYXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of pairs of (character name, pre-processed character) \n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n",
        "train_labels = [name for name, doc in training_corpus]"
      ],
      "metadata": {
        "id": "fQbjOe6PTYXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_character_docs = create_character_document_from_dataframe(val_data, max_line_count=40)\n",
        "print('Num. Characters: ',len(val_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in val_character_docs.keys():\n",
        "    print(name, 'Num of Words: ',len(val_character_docs[name].split()))\n",
        "    total_words += len(val_character_docs[name].split())\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "val_corpus = [(name, pre_process(doc)) for name, doc in sorted(val_character_docs.items())]\n",
        "val_labels = [name for name, doc in val_corpus]"
      ],
      "metadata": {
        "id": "fy9FjRc_TYXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 TF-IDF using count, POS tag and one previous words"
      ],
      "metadata": {
        "id": "AjgxgKZnaX6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  #simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for previous word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "            \n",
        "       # For POS tag\n",
        "            pos_list=[] \n",
        "            pos_list.append(pos_tag([character_doc[i]])) \n",
        "            #pos_list format will be [[(\"hello\",\"NN\"),(\"go\",\"JJ\")]]\n",
        "            pos_dict={}\n",
        "            for m in pos_list:\n",
        "              pos_dict[m[0][0]]=m[0][1]\n",
        "            #pos_dict format will be {\"hello\": \"NN\", \"go\": \"JJ\"}\n",
        "            for key_1,value_1 in  pos_dict.items():\n",
        "              lis_values.append(\"POS_\" + str(value_1))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "\n",
        "\n",
        "    for i in list(features_dict.keys()): #Deleting _eol_ key as it doesn't have much impact as it is used just for indicating next line\n",
        "      if i=='_eol_':\n",
        "        del features_dict[i]\n",
        "\n",
        "    print(features_dict)\n",
        "    return (features_dict)"
      ],
      "metadata": {
        "id": "azvBjkpDaX6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "cJ3jKjtvaX6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "GnxuL_OGaX6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf[\"Count + ONE previous word+ POS tag\"]=mean_rank"
      ],
      "metadata": {
        "id": "kPi6HYd4aX6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf"
      ],
      "metadata": {
        "id": "VutV8-aTd-uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 TF-IDF using count POS, one previous word and one next word"
      ],
      "metadata": {
        "id": "OC3UgbFVaX6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_2(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "   \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for now a simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      \n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "\n",
        "            if ((i>=len(character_doc)-1)|(character_doc[i]=='_eol_')): #for post word\n",
        "              next_word=\" \"\n",
        "            else:\n",
        "              next_word=character_doc[i+1]\n",
        "              if next_word=='_eol_':\n",
        "                next_word=\" \"\n",
        "            lis_values.append(\"POST_\" +str(next_word))\n",
        "            \n",
        "\n",
        "            pos_list=[]\n",
        "            pos_list.append(pos_tag([character_doc[i]]))\n",
        "\n",
        "            pos_dict={}\n",
        "            for m in pos_list:\n",
        "              pos_dict[m[0][0]]=m[0][1]\n",
        "            for key_1,value_1 in  pos_dict.items():\n",
        "              lis_values.append(\"POS_\" + str(value_1))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "\n",
        "      for i in list(features_dict.keys()):\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "MOIwh5nOaX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary_2(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_2(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "F_08LJoYaX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "QmIrYRpKaX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf[\"Count + ONE previous word+ ONE next word+ POS tag\"]=mean_rank"
      ],
      "metadata": {
        "id": "2X7T41vZaX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 TF-IDF using count, One previous and one next word"
      ],
      "metadata": {
        "id": "-KuXEkv9aX6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_3(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for now a simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      \n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "            if ((i>=len(character_doc)-1)|(character_doc[i]=='_eol_')): #for post word\n",
        "              next_word=\" \"\n",
        "            else:\n",
        "              next_word=character_doc[i+1]\n",
        "              if next_word=='_eol_':\n",
        "                next_word=\" \"\n",
        "            lis_values.append(\"POST_\" +str(next_word))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "      for i in list(features_dict.keys()):\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "dCWzSuLgaX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary_3(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_3(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "kJkMBrgBaX6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "BWSGwVvXaX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf[\"Count +ONE previous word + ONE next word\"]=mean_rank"
      ],
      "metadata": {
        "id": "eGs0M3_oaX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Bigrams:- TF-IDF using count and Two previous words"
      ],
      "metadata": {
        "id": "q5VHtkgCaX7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_4(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for simple count\n",
        "    counts=dict(counts)\n",
        "\n",
        "\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word_1=\" \"\n",
        "              prev_word_2=\" \"\n",
        "            elif (character_doc[i-2]==character_doc[-1])|(character_doc[i-2]=='_eol_'):\n",
        "              prev_word_1=character_doc[i-1]\n",
        "              prev_word_2=\" \"\n",
        "            else:\n",
        "              prev_word_1=character_doc[i-1]\n",
        "              prev_word_2=character_doc[i-2]\n",
        "            lis_values.append(\"PRE1_\"+str(prev_word_1))\n",
        "            lis_values.append(\"PRE2_\"+str(prev_word_2))\n",
        "\n",
        "\n",
        "            pos_list=[] #Pos tag\n",
        "            pos_list.append(pos_tag([character_doc[i]]))\n",
        "\n",
        "            pos_dict={}\n",
        "            for m in pos_list:\n",
        "              pos_dict[m[0][0]]=m[0][1]\n",
        "            for key_1,value_1 in  pos_dict.items():\n",
        "              lis_values.append(\"POS_\" + str(value_1))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "      for i in list(features_dict.keys()):\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "eUGDsvm5aX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary_4(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_4(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "ZstfjLyBaX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "HBulub4maX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf[\"Count + Two previous word(Bi-grams)\"]=mean_rank"
      ],
      "metadata": {
        "id": "6uJZ_3P5aX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 TF-IDF Testing on best feature combination:\n",
        "i.e \"Count + ONE previous word+ ONE next word+ POS tag(only one)\""
      ],
      "metadata": {
        "id": "KU8DhGHNaX7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "def to_feature_vector_dictionary_5(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "      \n",
        "    features_list=[]\n",
        "    features_dict={}\n",
        "    counts = Counter(character_doc)  # for simple count\n",
        "    counts=dict(counts)\n",
        "   \n",
        "\n",
        "    pos=dict(pos_tag(character_doc))\n",
        "    features_dict={}\n",
        "    for key, value in counts.items():\n",
        "      lis_values=[]\n",
        "      lis_values.append(\"Count_\"+str(value))\n",
        "      if key in pos:\n",
        "        lis_values.append(\"POS_\"+str(pos[key]))\n",
        "      \n",
        "      for i in range(0,len(character_doc)):\n",
        "        if(key==character_doc[i]):\n",
        "            if (character_doc[i-1]==character_doc[-1])|(character_doc[i-1]=='_eol_'): #for prev word\n",
        "              prev_word=\" \"\n",
        "            else:\n",
        "              prev_word=character_doc[i-1]\n",
        "            lis_values.append(\"PRE_\"+str(prev_word))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "            if ((i>=len(character_doc)-1)|(character_doc[i]=='_eol_')): #for post word\n",
        "              next_word=\" \"\n",
        "            else:\n",
        "              next_word=character_doc[i+1]\n",
        "              if next_word=='_eol_':\n",
        "                next_word=\" \"\n",
        "            lis_values.append(\"POST_\" +str(next_word))\n",
        "            features_dict[key]=lis_values\n",
        "\n",
        "      for i in list(features_dict.keys()):\n",
        "        if i=='_eol_':\n",
        "          del features_dict[i]\n",
        "    print(features_dict)\n",
        "    return (features_dict)\n"
      ],
      "metadata": {
        "id": "nuLYYtNIaX7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary_5(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary_5(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "lXuI95zYaX7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, val_feature_matrix, train_labels, val_labels)"
      ],
      "metadata": {
        "id": "ePUenahDaX7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf[\"Count + ONE previous word+ ONE next word+ POS tag (only one)\"]=mean_rank"
      ],
      "metadata": {
        "id": "3PLPPxfmaX7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "9cQ7Xp5naX7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_tfidf"
      ],
      "metadata": {
        "id": "0N9kPtXnaX7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pylab import *\n",
        "fig = figure()\n",
        "names = list(features_tfidf.keys())\n",
        "values = list(features_tfidf.values())\n",
        "plt.bar(0,values[0],tick_label=names[0])\n",
        "plt.bar(1,values[1],tick_label=names[1])\n",
        "plt.bar(2,values[2],tick_label=names[2])\n",
        "plt.bar(3,values[3],tick_label=names[3])\n",
        "plt.bar(4,values[4],tick_label=names[4])\n",
        "plt.bar(5,values[5],tick_label=names[5])\n",
        "plt.xticks(range(0,6),names)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xticklabels(features_tfidf.keys(),rotation=90) ;\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8H25ze7OaX7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result:\n",
        "\n",
        "As it can observed from above \n",
        "\n",
        "Applying TF-IDF on q3 feature. i.e scene_info + previous line + current line + post line gave the best mean rank of 1.06"
      ],
      "metadata": {
        "id": "9EaCVpW4ZBvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Select and test the best vector representation method (10 marks)\n",
        "Finish the optimization of your vector representations by selecting the best combination of the techniques you tried in Q1-3 and test using the code below to train on all of the training data (using the first 400 lines per character maximum) and do the final testing on the test file (using the first 40 lines per character maximum).\n",
        "\n",
        "Make any neccessary adjustments such that it runs in the same way as the training/testing regime you developed above- e.g. making sure any transformer objects are initialized before `create_document_matrix_from_corpus` is called. Make sure your best system is left in the notebook and it is clear what the mean rank, accuracy of document selection are on the test data."
      ],
      "metadata": {
        "id": "3d5U91gAuKP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Feature used: \" Count + ONE previous word+ ONE next word+ POS tag(only one)\"\n",
        "Since we received best mean rank(minimum) for feature \" Count + ONE previous word+ ONE next word+ POS tag(only one)\". Therefore I'll be using this feature for test data"
      ],
      "metadata": {
        "id": "A_96xwnJ5tqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in training data and display in pandas dataframe\n",
        "train_path='training.csv'\n",
        "all_train_data = pd.read_csv(train_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "test_path ='test.csv'\n",
        "test_data = pd.read_csv(test_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "\n",
        "# Inspect\n",
        "all_train_data"
      ],
      "metadata": {
        "id": "aaQItVC0vhjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsiode_scene_column = all_train_data.Episode.astype(str) + \"-\" + all_train_data.Scene.astype(str)\n",
        "all_train_data['episode_scene'] = epsiode_scene_column\n",
        "\n",
        "\n",
        "epsiode_scene_column = test_data.Episode.astype(str) + \"-\" + test_data.Scene.astype(str)\n",
        "test_data['episode_scene'] = epsiode_scene_column\n"
      ],
      "metadata": {
        "id": "F92D_nGR0s--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data['Scene_info'].value_counts().head(12).plot(kind='bar')"
      ],
      "metadata": {
        "id": "gkxQIqzr0fsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data"
      ],
      "metadata": {
        "id": "Vm2ytPHs7TM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data['Character_name'].value_counts().plot(kind='bar') "
      ],
      "metadata": {
        "id": "cjnhRRDS7SDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names=all_train_data['Character_name'].unique()\n",
        "names_dict={}\n",
        "for i in names:\n",
        "  k=all_train_data.loc[all_train_data['Character_name']==i]\n",
        "  names_dict[i]=np.array(k['Gender'].head(1))\n",
        "print(names_dict)"
      ],
      "metadata": {
        "id": "DhP_5zFq7gnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_dict_gen={} #making dic with character name and it's gender\n",
        "\n",
        "for key,value in names_dict.items():\n",
        "  names_dict_gen[key]=value[0]"
      ],
      "metadata": {
        "id": "CAhNYSwWOAq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_dict_gen"
      ],
      "metadata": {
        "id": "x3fBs81hPapB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del names_dict_gen[\"OTHER\"] #Since OTHER gender is mixed so we deleted it \n",
        "names_dict_gen"
      ],
      "metadata": {
        "id": "dSrotspHPlY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_count_dict={}\n",
        "gender_list=['FEMALE', 'MALE']\n",
        "for i in gender_list:\n",
        "  gender_count_dict[i]=list(names_dict_gen.values()).count(i)"
      ],
      "metadata": {
        "id": "IDUcnrDtSgPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gender_count_dict"
      ],
      "metadata": {
        "id": "uXJgRTQLS2Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = figure()\n",
        "names = list(gender_count_dict.keys())\n",
        "values = list(gender_count_dict.values())\n",
        "plt.bar(0,values[0],tick_label=names[0])\n",
        "plt.bar(1,values[1],tick_label=names[1])\n",
        "plt.xticks(range(0,2),names)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xticklabels(gender_count_dict.keys(),rotation=90) ;\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l98tDlTDwOxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data['Scene_info'].value_counts().head(3)"
      ],
      "metadata": {
        "id": "Kb9QQ3I0xlR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Data Analysis observed for all_train_data:\n",
        "\n",
        "a. Top three scene are \"CAFE_INT_DAY_LIGHT\", \"VIC_DOWNSTAIRS_INT_DAY_LIGHT\" and \"BRIDGE_STREET_EXT_DAY_LIGHT\".\n",
        "\n",
        "b. There are totally 8 Female main character and 7 Male main character. Excluding OTHER as they are of both gender"
      ],
      "metadata": {
        "id": "pNlwI50cxZ3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import string\n",
        "import re\n",
        "def pre_process(character_text):\n",
        "    \"\"\"Pre-process all the concatenated lines of a character, \n",
        "    using tokenization, spelling normalization and other techniques.\n",
        "    \n",
        "    Initially just a tokenization on white space. Improve this for Q1.\n",
        "    \n",
        "    ::character_text:: a string with all of one character's lines\n",
        "    \"\"\"\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer() #initializing lemmatizer\n",
        "    character_text=character_text.lower() #converting to all lower case\n",
        "    pattern = r\"\\s+|(?<=\\s)'|'(?=\\s)|'(?!.*\\.\\.) |(?<=\\w)([,..!/?])\" #different pattern to be used for splitting\n",
        "    result = [s for s in re.split(pattern, character_text) if s] \n",
        "    stop_words = stopwords.words('english') #usage of stopwords\n",
        "    punctuation=list(string.punctuation) #usage of punctuation to delete unwanted punctuation\n",
        "    punctuation.append(\"..\") #required punctuation has been appended\n",
        "    stop=stop_words+punctuation #now all words excepted stopwords and unwanted punctuation will be ignored\n",
        "    filtered_sentence = [w for w in result if not w in stop] \n",
        "    for i in filtered_sentence:\n",
        "        if i.isnumeric()==True:\n",
        "          indx=filtered_sentence.index(i)\n",
        "          k=num2words.num2words(i) #trying to convert numeric to word\n",
        "          filtered_sentence[indx]=k\n",
        "    lem_sent=[] \n",
        "    for i in filtered_sentence: #looping through each pre-processed word present in filtered_sentence\n",
        "      lem_sent.append(lemmatizer.lemmatize(i)) #Lemmitizing the word\n",
        "\n",
        "    return lem_sent"
      ],
      "metadata": {
        "id": "jrj8iXtNuKQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data.isnull().sum() #12 null values in all_train_data"
      ],
      "metadata": {
        "id": "R_MxjfHzuKQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.isnull().sum() #1 null value in test_data"
      ],
      "metadata": {
        "id": "FcfELcSquKQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping null values\n",
        "all_train_data.dropna(inplace=True)\n",
        "test_data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "TYUEprnquKQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "pUc8qug0uKQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.isnull().sum()"
      ],
      "metadata": {
        "id": "ZVSp7U3tyBM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data"
      ],
      "metadata": {
        "id": "FGWFuwPFuKQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english') #stopword and punctuation to be removed from \"Line\" column\n",
        "punctuation=list(string.punctuation)\n",
        "punctuation.append(\"..\")\n",
        "stop=stop_words+punctuation"
      ],
      "metadata": {
        "id": "FhVfX2OZuKQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data['Line'] = all_train_data['Line'].apply(word_tokenize)\n",
        "all_train_data['Line'] = all_train_data['Line'].apply(lambda words: [word for word in words if word not in stop])"
      ],
      "metadata": {
        "id": "QtQdfPGUuKQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['Line'] = test_data['Line'].apply(word_tokenize)\n",
        "test_data['Line'] = test_data['Line'].apply(lambda words: [word for word in words if word not in stop])"
      ],
      "metadata": {
        "id": "bVqYtQsLuKQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Joining words after tokenizing in train_data\n",
        "no_stop=[]\n",
        "for i in all_train_data['Line']:\n",
        "  k=' '.join(i)\n",
        "  no_stop.append(k)\n",
        "all_train_data['Line']=no_stop"
      ],
      "metadata": {
        "id": "lWaNYtliuKQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Joining words after tokenizing in test_data\n",
        "no_stop=[]\n",
        "for i in test_data['Line']:\n",
        "  k=' '.join(i)\n",
        "  no_stop.append(k)\n",
        "test_data['Line']=no_stop"
      ],
      "metadata": {
        "id": "jUZEYLxtuKQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data"
      ],
      "metadata": {
        "id": "B_QQLq4OuKQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "vd0FSec3uKQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data = all_train_data.reset_index() #reseting index in all_train_data\n",
        "all_train_data.drop(['index'],axis=1,inplace=True)\n",
        "all_train_data"
      ],
      "metadata": {
        "id": "wjmeEDwduKQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.reset_index() #resetting index in test_data\n",
        "test_data.drop(['index'],axis=1,inplace=True)\n",
        "test_data"
      ],
      "metadata": {
        "id": "xlzwJ3TIuKQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one document per character\n",
        "def create_character_document_from_dataframe(df, max_line_count):\n",
        "    \"\"\"Returns a dict with the name of the character as key,\n",
        "    their lines joined together as a single string, with end of line EOL\n",
        "    markers between them.\n",
        "    \n",
        "    ::max_line_count:: the maximum number of lines to be added per character\n",
        "    \"\"\"\n",
        "    #incorporates the context of the line spoken by the characters in terms of the lines spoken by other characters in the same scene (immediately before and after)\n",
        "    result = []\n",
        "    \n",
        "    Scene_info_list=[]\n",
        "    for i in df['Scene_info']: # few Scene_info \"SLATERS', KITCHEN/FRONT ROOM INT DAY LIGHT\" are in this format\n",
        "      k=i.replace(\",\",\"\") \n",
        "      k=k.replace(' ','_')# therefore making it into \"SLATERS'_KITCHEN/FRONT_ROOM_INT_DAY_LIGHT\" format\n",
        "      Scene_info_list.append(k)\n",
        "    df['Scene_info']=Scene_info_list\n",
        "    episodeCol = df.Episode\n",
        "    sceneCol = df.Scene\n",
        "    lineCol = df.Line\n",
        "    charname=df.Character_name\n",
        "    episcene= df.episode_scene\n",
        "    scene_info=df.Scene_info\n",
        "\n",
        "\n",
        "    currEpisode = episodeCol[0]\n",
        "    currScene = sceneCol[0]\n",
        "      \n",
        "    for i in range(0, len(episodeCol)): \n",
        "        temp = []\n",
        "\n",
        "        ep = episodeCol[i]\n",
        "        scene = sceneCol[i]\n",
        "        line = lineCol[i]\n",
        "        name=charname[i]\n",
        "        episcene_con=episcene[i]\n",
        "        scene_info_con=scene_info[i]\n",
        "\n",
        "        lineList = line.split()\n",
        "\n",
        "        if(i != 0): #For previous character line\n",
        "          prevEp = episodeCol[i-1]\n",
        "          prevScene = sceneCol[i-1]\n",
        "          prevLine = lineCol[i-1]\n",
        "          prevChar = charname[i-1]\n",
        "        else:\n",
        "          prevEp = -1\n",
        "          prevScene = -1\n",
        "          prevLine = 'NONE'  \n",
        "          prevChar = -1      \n",
        "\n",
        "\n",
        "        prevLineList = prevLine.split()\n",
        "\n",
        "        if(i != len(episodeCol)-1): #for post character line\n",
        "          nextEp = episodeCol[i+1]\n",
        "          nextScene = sceneCol[i+1]\n",
        "          nextLine = lineCol[i+1]  \n",
        "          nextChar = charname[i+1]\n",
        "        else:\n",
        "          nextEp = -1\n",
        "          nextScene = -1\n",
        "          nextLine = 'NONE'  \n",
        "          nextChar = -1\n",
        "\n",
        "\n",
        "        nextLineList = nextLine.split()\n",
        "\n",
        "\n",
        "        if(ep == prevEp and scene == prevScene and name!=prevChar): \n",
        "            append_str = 'PRE_'\n",
        "            pre_res = [append_str + sub for sub in prevLineList]\n",
        "            temp.append(scene_info_con)\n",
        "            temp.extend(pre_res)\n",
        "        else:\n",
        "            temp.append(scene_info_con)\n",
        "            temp.append('NONE')\n",
        "        \n",
        "\n",
        "        temp.extend(lineList)\n",
        "\n",
        "        if(ep == nextEp and scene == nextScene and name!=nextChar):\n",
        "            append_str = 'POST_'\n",
        "            pre_res = [append_str + sub for sub in nextLineList]\n",
        "            temp.extend(pre_res)\n",
        "\n",
        "        else:\n",
        "            temp.append('NONE')\n",
        "        \n",
        "        result.append(temp)\n",
        "        \n",
        "\n",
        "    df['pre_current_post']=result #make new column in datframe with pre line current line and post line\n",
        "    uni_dict={} #initialise unique dict\n",
        "    line_list=[] #initialise list\n",
        "    char_name=df['Character_name'].unique() #getting unique char name\n",
        "    for i in char_name: #for each char unique dataframe\n",
        "      char_name_dataframe=df.loc[df['Character_name']==i]\n",
        "      for pre_post in char_name_dataframe.pre_current_post:\n",
        "        pre_post.append('_EOL_') #EOL to know current line ending\n",
        "        line_list=line_list + pre_post\n",
        "        uni_dict[i]=line_list\n",
        "      line_list=[]\n",
        "\n",
        "    print(uni_dict)\n",
        "    return uni_dict"
      ],
      "metadata": {
        "id": "sh2agJgruKQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the number of words each character has in the training set\n",
        "# only use the first 360 lines of each character\n",
        "train_character_docs = create_character_document_from_dataframe(all_train_data, max_line_count=400)\n",
        "for k,v in train_character_docs.items():\n",
        "  train_character_docs[k]=\",\".join(v)\n",
        "#train_character_docs\n",
        "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in train_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(train_character_docs[name].split(',')))\n",
        "    total_words += len(train_character_docs[name].split())\n",
        "print(\"total words\", total_words)"
      ],
      "metadata": {
        "id": "7G_qiGLTuKQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_data"
      ],
      "metadata": {
        "id": "QzfymegjuKQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_character_docs"
      ],
      "metadata": {
        "id": "kCHJ_KRJuKQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of pairs of (character name, pre-processed character) \n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n",
        "train_labels = [name for name, doc in training_corpus]"
      ],
      "metadata": {
        "id": "vlhjITbHuKQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "6E-KC6SWuKQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_character_docs = create_character_document_from_dataframe(test_data, max_line_count=40)\n",
        "for k,v in test_character_docs.items():\n",
        "  test_character_docs[k]=\",\".join(v)\n",
        "test_character_docs\n",
        "print('Num. Characters: ',len(test_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in test_character_docs.keys():\n",
        "    print(name, 'Num of Words: ',len(test_character_docs[name].split(',')))\n",
        "    total_words += len(test_character_docs[name].split(','))\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "test_corpus = [(name, pre_process(doc)) for name, doc in sorted(test_character_docs.items())]\n",
        "test_labels = [name for name, doc in test_corpus]"
      ],
      "metadata": {
        "id": "0RCwwt-HuKQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_feature_vector_dictionary(character_doc, extra_features=[]):\n",
        "    \"\"\"Converts a list of pre-processed tokens and extra features\n",
        "    to a Dictionary as a function of the tokens.\n",
        "    \n",
        "    Initially just a simple count. Improve this for Q2.\n",
        "    \n",
        "    ::character_doc:: a list of pre-processed tokens\n",
        "    ::extra_features:: any extra features for the character to be added to feature vector dict\n",
        "    \"\"\"\n",
        "    counts = Counter(character_doc)  # for now a simple count\n",
        "    counts = dict(counts)\n",
        "    # add the extra features, for now just adding one count for each extra feature\n",
        "    for feature in extra_features:\n",
        "        counts[feature] += 1\n",
        "    \n",
        "    for i in list(counts.keys()): #Deleting none and eol key as it is used just for no previous or post line and end of sentence \n",
        "      if i=='none' or i=='_eol_':\n",
        "        del counts[i]\n",
        "    print(counts)\n",
        "    return counts  "
      ],
      "metadata": {
        "id": "D7egtMCmuKQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Without TF-IDF"
      ],
      "metadata": {
        "id": "ugNXeGqEFvzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "test_feature_matrix = create_document_matrix_from_corpus(test_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "1_UT06eKuKQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, test_feature_matrix, train_labels, test_labels)"
      ],
      "metadata": {
        "id": "j01hJzyeuKQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_test={}\n",
        "all_train_test[\"Scene_info + previous line + current line + post line (WITHOUT TF-IDF)\"]=mean_rank"
      ],
      "metadata": {
        "id": "ir2flxCgylsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 With TF-IDF"
      ],
      "metadata": {
        "id": "urb1YEP48VPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "corpusVectorizer = Pipeline([('count', DictVectorizer()),\n",
        "                 ('tfid', TfidfTransformer(sublinear_tf= True))])\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "      corpusVectorizer.fit([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, fitting=True)\n",
        "test_feature_matrix = create_document_matrix_from_corpus(test_corpus, fitting=False)"
      ],
      "metadata": {
        "id": "3Oc0rQlauRS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, test_feature_matrix, train_labels, test_labels)"
      ],
      "metadata": {
        "id": "lRla0zs5uRS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_test[\"Scene_info + previous line + current line + post line (WITH TF-IDF)\"]=mean_rank"
      ],
      "metadata": {
        "id": "mFmmmupFuRS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_test"
      ],
      "metadata": {
        "id": "uFLnhMj0y3oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result:\n",
        "Achieved 1 mean rank for feature 'Scene_info + previous line + current line + post line (WITH TF-IDF)'"
      ],
      "metadata": {
        "id": "gElkZ44I6gO0"
      }
    }
  ]
}